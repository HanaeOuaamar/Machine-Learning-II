# ğŸš¦   Machine Learning â€” SÃ©rie de Travaux Pratiques

Bienvenue dans ce repository qui regroupe une sÃ©rie de 4 Travaux Pratiques (TP) autour du **Machine Learning** et de l'**Apprentissage par Renforcement** rÃ©alisÃ©s dans le cadre du cours Ã  l'Ã‰cole Nationale de l'Intelligence Artificielle et du Digital Berkane. AccompagnÃ©s d'un environnement de simulation de gestion du trafic routier.

---

## ğŸ“„ Description

L'objectif de ce projet est d'appliquer des concepts fondamentaux de l'intelligence artificielle et de l'apprentissage automatique Ã  travers 4 TPs progressifs. Les TPs couvrent diffÃ©rents aspects tels que :
- Comprendre la structure d'un environnement Gym en explorant des propriÃ©tÃ©s et ses actions possibles
- ContrÃ´le manuel de l'agent
-  ImplÃ©mentation de Q-learning et SARSA
-  Mise Ã  jour de la politique avec PPO

Le fichier **`env_traffic.py`** fournit un environnement de simulation simplifiÃ© permettant d'expÃ©rimenter et d'Ã©valuer des stratÃ©gies de gestion du trafic.

---

## ğŸ—‚ï¸ Contenu du Repository
ğŸ“‚ TPs-Machine-Learning-II

â”œâ”€â”€ ğŸ“„ README.md  
â”œâ”€â”€ TP1.ipynb  
â”œâ”€â”€ TP2.ipynb   
â”œâ”€â”€ TP3.ipynb
â”œâ”€â”€ TP4.ipynb    
â”œâ”€â”€ env_traffic.py  â†’ Simulation d'environnement de trafic
â”œâ”€â”€ requirements.txt    â†’ DÃ©pendances Python


---

# ğŸ“Œ A propos des TPs:
## TP1:  Introduction Ã  l'Apprentissage par Renforcement et dÃ©couverte d'OpenAI Gym
### ğŸ¯ Objectif:
 L'objectif de ce TP est de se familiariser avec les outils de base du Reinforcement Learning (RL) en utilisant
 la bibliothque OpenAI Gymnasium. en dÃ©couvrant : 
  - la structure d'un environnement Gym,
  - l'espace des actions et observations,
  - les interactions agent-environnement,
  - l'importance d'une politique efficace,
    
    ### ğŸ“ Fichier : TP_01.ipynb
    #### InterprÃ©tation:
    
    - Un agent qui agit au hasard ne peut pas maintenir le systÃ¨me stable sur le long terme.

    - Pour amÃ©liorer les performances, il est nÃ©cessaire de dÃ©velopper une politique intelligente basÃ©e sur l'apprentissage par renforcement.

    - Le contrÃ´le manuel est utile pour apprÃ©hender les consÃ©quences des actions.

    - L'environnement CartPole est simple mais trÃ¨s efficace pour illustrer les principes de lâ€™apprentissage par renforcement (Ã©tats, actions, rÃ©compenses, Ã©pisodes...).

    ## TP2:  Apprentissage par Q-Learning avec FrozenLake
    ### ğŸ¯ Objectif:
    
    Ce TP a pour but de mettre en pratique les fondements du Q-Learning Ã  travers l'environnement FrozenLake de Gymnasium.

    L'objectif est de permettre Ã  un agent d'apprendre Ã  atteindre la cible sans tomber dans l'eau, en maximisant les rÃ©compenses.
    
     ### ğŸ“ Fichier : TP_02.ipynb
     #### InterprÃ©tation:
    
    Fonctions utiles:
    Q-Learning : Q(s,a) â† Q(s,a) + Î±[r + Î³ maxâ‚ Q(s',a') - Q(s,a)]
    
    SARSA	:   Q(s,a) â† Q(s,a) + Î±[r + Î³ Q(s',a') - Q(s,a)]
    
    Ce TP montre comment un agent peut apprendre Ã  maximiser sa rÃ©compense dans un environnement incertain.

    Le Q-learning permet, grÃ¢ce aux essais et erreurs, d'optimiser progressivement une stratÃ©gie sans modÃ¨le de l'environnement.

    L'environnement FrozenLake constitue un excellent terrain d'expÃ©rimentation pour comprendre les bases de l'apprentissage par renforcement.

    ## TP3:  Optimisation des Feux de Circulation par Reinforcement Learning
    ### ğŸ¯ Objectif:
    
    Lâ€™objectif de ce TP est dâ€™optimiser la gestion des feux de circulation Ã  lâ€™aide des algorithmes Q-Learning et SARSA. Il vise Ã  comparer ces deux mÃ©thodes dâ€™apprentissage 
    par renforcement afin dâ€™analyser leur impact sur la fluiditÃ© du trafic.

    ### ğŸ“ Fichier : TP_03.ipynb
    #### ğŸ›  Changements effectuÃ©s dans le TP
   ğŸ”§ 1. Correction des erreurs d'index:
      L'environnement gÃ©nÃ¨re des Ã©tats avec des valeurs pouvant dÃ©passer 9 (Ã  cause de self.state += new_cars), alors que la q_table est dÃ©finie pour des Ã©tats compris 
      entre 0 et 9.
      âœ… Correction apportÃ©e :
      Ajout de la ligne suivante dans env_traffic.py :
      self.state = np.clip(self.state, 0, 9)
      âœ… Application supplÃ©mentaire du clip dans les fonctions train_q_learning() et train_sarsa() :
      state = tuple(np.clip(env.reset(), 0, 9))
      next_state = tuple(np.clip(next_state_raw, 0, 9)

  #### InterprÃ©tation:
  Q-Learning est adaptÃ© si l'on souhaite maximiser rapidement la performance, quitte Ã  prendre plus de risques dans lâ€™apprentissage.
  SARSA est idÃ©al pour un apprentissage plus sÃ»r et plus stable.
  Les deux mÃ©thodes peuvent convenir Ã  lâ€™optimisation du trafic, selon les objectifs (efficacitÃ© vs prudence).    lisez ca et ecrivez moi un objevtif de ce tp.

 ## TP4: Proximal Policy Optimization (PPO)

### ğŸ¯ Objectif:
Lâ€™objectif de PPO dans ce TP est dâ€™apprendre Ã  un agent Ã  prendre de meilleures dÃ©cisions (actions) dans un environnement, en mettant Ã  jour sa politique (probabilitÃ©s dâ€™actions) et sa valeur des Ã©tats de maniÃ¨re progressive et stable.

 ### ğŸ“ Fichier : TP_04.ipynb
 
   #### InterprÃ©tation:
   
AprÃ¨s l'entraÃ®nement avec PPO, on constate que l'agent amÃ©liore progressivement sa politique. GrÃ¢ce au mÃ©canisme de clipping, les mises Ã  jour sont modÃ©rÃ©es, Ã©vitant des modifications trop violentes. Les probabilitÃ©s des bonnes actions augmentent naturellement, et la valeur des Ã©tats reflÃ¨te mieux leur utilitÃ© future. Lâ€™agent devient donc plus compÃ©tent Ã Â chaqueÂ Ã©pisode.

## ğŸš€ Installation & ExÃ©cution

### 1. Cloner le repository


```bash
git clone https://github.com/HanaeOuaamar/TPs-Machine-learning-II.git
cd votre-repository
```

### 2. Installer les dÃ©pendances

```bash
pip install -r requirements.txt
```
### 3. ExÃ©cuter les notebooks 
```bash
jupyter notebook
```





    


## ğŸ‘¥ Auteurs
RÃ©alisÃ© par : [Hanae Ouaamar]

EncadrÃ© par : [Mr Mohamed Khalifa Boutahir]

UniversitÃ© : [ENIAD Berkane ]









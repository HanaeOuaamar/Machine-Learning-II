{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "267c31e2-baae-4617-b883-08bc98e9fc83",
   "metadata": {},
   "source": [
    "<div style=\"border: 2px solid black; padding: 10px; background-color: #f7f7f7; text-align: center;\">\n",
    "    <h1 style=\"color: Navy; text-decoration: underline;\">TP2: Machine Learning II</h1>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ba9f36a-49a5-4de6-a57d-455d0dc5a3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781ba5c2-732c-4ff9-8354-12345f697d35",
   "metadata": {},
   "source": [
    "####   Exercie1 : Découverte et exploration d’un environnement Gym\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "accd5793-6a6e-4071-bccc-7389738b6d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Espace des états : 16\n",
      "Espace des actions : 4\n",
      "\n",
      "Épisode 1\n",
      "État : 0, Action : 0, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 0, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 3, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 2, Nouveau état : 1, Récompense : 0.0\n",
      "État : 1, Action : 0, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 1, Nouveau état : 1, Récompense : 0.0\n",
      "État : 1, Action : 0, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 1, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 2, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 1, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 1, Nouveau état : 4, Récompense : 0.0\n",
      "État : 4, Action : 1, Nouveau état : 4, Récompense : 0.0\n",
      "État : 4, Action : 3, Nouveau état : 5, Récompense : 0.0\n",
      "\n",
      "Épisode 2\n",
      "État : 0, Action : 0, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 2, Nouveau état : 1, Récompense : 0.0\n",
      "État : 1, Action : 2, Nouveau état : 5, Récompense : 0.0\n",
      "\n",
      "Épisode 3\n",
      "État : 0, Action : 3, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 2, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 3, Nouveau état : 1, Récompense : 0.0\n",
      "État : 1, Action : 0, Nouveau état : 5, Récompense : 0.0\n",
      "\n",
      "Épisode 4\n",
      "État : 0, Action : 3, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 2, Nouveau état : 0, Récompense : 0.0\n",
      "État : 0, Action : 1, Nouveau état : 1, Récompense : 0.0\n",
      "État : 1, Action : 1, Nouveau état : 2, Récompense : 0.0\n",
      "État : 2, Action : 1, Nouveau état : 6, Récompense : 0.0\n",
      "État : 6, Action : 1, Nouveau état : 5, Récompense : 0.0\n",
      "\n",
      "Épisode 5\n",
      "État : 0, Action : 1, Nouveau état : 4, Récompense : 0.0\n",
      "État : 4, Action : 0, Nouveau état : 4, Récompense : 0.0\n",
      "État : 4, Action : 2, Nouveau état : 5, Récompense : 0.0\n"
     ]
    }
   ],
   "source": [
    "# Création de l’environnement FrozenLake\n",
    "env =gym.make(\"FrozenLake-v1\",is_slippery=True)\n",
    "# Affichage des espaces\n",
    "print(\"Espace des états :\", env.observation_space.n)\n",
    "print(\"Espace des actions :\", env.action_space.n)\n",
    "\n",
    "# Exécution d’actions aléatoires\n",
    "for episode in range(5):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    print(f\"\\nÉpisode {episode+1}\")\n",
    "    while not done:\n",
    "        action = env.action_space.sample()\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        print(f\"État : {state}, Action : {action}, Nouveau état : {new_state}, Récompense : {reward}\")\n",
    "        state = new_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e1bc15-5357-4a84-a5e6-3ab0dc0a9238",
   "metadata": {},
   "source": [
    "#### Exercice 2 : Implémentation de la Q-Table: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2703bcf-a1d9-4adc-b938-6fa281ff97b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table initialisée :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Initialisation de la Q-Table (n_states x n_actions)\n",
    "q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Affichage de la Q-Table initiale\n",
    "print(\"Q-Table initialisée :\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102af2b-20a2-46db-8779-ab9c90670d0f",
   "metadata": {},
   "source": [
    "#### Exercice 3 : Implémentation de Q-learning avec mise à jour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24f0cace-e293-4605-8e21-9b1fb5896ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-Table après apprentissage :\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# Hyperparamètres\n",
    "alpha = 0.1           # taux d’apprentissage\n",
    "gamma = 0.99          # facteur de discount\n",
    "epsilon = 1.0         # exploration initiale\n",
    "epsilon_decay = 0.995\n",
    "min_epsilon = 0.01\n",
    "num_episodes = 5000\n",
    "\n",
    "# Entraînement de l’agent\n",
    "for episode in range(num_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Choix de l’action : exploration ou exploitation\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :])\n",
    "\n",
    "        # Exécution de l’action\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        # Mise à jour de la Q-Table\n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[new_state, :])\n",
    "        new_value = old_value + alpha * (reward + gamma * next_max - old_value)\n",
    "        q_table[state, action] = new_value\n",
    "\n",
    "        state = new_state\n",
    "\n",
    "    # Mise à jour du taux d’exploration\n",
    "    epsilon = max(min_epsilon, epsilon * epsilon_decay)\n",
    "\n",
    "# Affichage final de la Q-Table\n",
    "print(\"Q-Table après apprentissage :\")\n",
    "print(q_table)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f0c6b97-65c6-4ec7-a221-f1890ef9f848",
   "metadata": {},
   "source": [
    "#### Exercice 4 : Évaluation des Performances de l’Agent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44706650-7d4b-40a3-ab99-3e1f528352d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taux de réussite sur 100 épisodes : 0%\n"
     ]
    }
   ],
   "source": [
    "num_test_episodes = 100\n",
    "successes = 0\n",
    "\n",
    "for _ in range(num_test_episodes):\n",
    "    state, info = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state, :])  # Action optimale\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "        if done and reward == 1:\n",
    "            successes += 1\n",
    "\n",
    "# Affichage du taux de réussite\n",
    "print(f\"Taux de réussite sur {num_test_episodes} épisodes : {successes}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
